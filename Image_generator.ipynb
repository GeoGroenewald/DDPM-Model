{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZL2qgW2BLXcY"
      },
      "source": [
        "# Data loading and diffusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "olk9Xz42LSur"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import ConcatDataset, DataLoader\n",
        "import torch\n",
        "import torchvision.utils as vutils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbivIoqaVUU9"
      },
      "source": [
        "##Load labelled data from STL10 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "togXHo4GwWk5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc92a816-c928-4502-9072-7d40c4863c78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:06<00:00, 24.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "transform = transforms.Compose([\n",
        "transforms.Resize((32, 32)),             # Resize any image to 96x96\n",
        "transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='.', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.CIFAR10(root='.', train=False, transform=transform, download=True)\n",
        "combined_dataset = ConcatDataset([train_dataset, test_dataset])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBKA_2xE2pfA",
        "outputId": "511df977-da8b-443d-c6f9-5178584c7fc7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 3, 32, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "x, __ = combined_dataset[0]\n",
        "testing = torch.unsqueeze(x,0)\n",
        "testing.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BP_3xBIVc-c"
      },
      "source": [
        "## Define all values and functions needed to diffuse a image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SpjNRreZLWBE"
      },
      "outputs": [],
      "source": [
        "#Define some diffusion schedule values that will be used in the diffusion forward pass\n",
        "#These values are computed uaing the cosine schedule\n",
        "T = 400\n",
        "s = 0.08\n",
        "t = torch.linspace(0,T,T+1, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
        "alpha_t = torch.cos((t/T + s)/(1 + s) * (torch.pi / 2)) ** 2\n",
        "alpha_t = alpha_t / alpha_t[0]\n",
        "beta_t = 1 - (alpha_t[1:] / alpha_t[:-1])  # β[1], β[2], ..., β[T]\n",
        "beta_t = torch.clip(beta_t, 1e-4, 0.999)\n",
        "beta_t = beta_t.type(torch.float32)\n",
        "alpha_cumprod = torch.cumprod(1-beta_t,dim=0)\n",
        "\n",
        "def forward_diff(images, time_steps):\n",
        "  #This function performs the diffusion of a normal recognizable batch of images and returns the diffused image as well as the noise for training\n",
        "  #this model uses the cosine schedule\n",
        "  #time step is used to define the diffusion at certain time steps\n",
        "\n",
        "  noise = torch.randn_like(images, device=images.device)\n",
        "\n",
        "  sqrt_alpha = torch.sqrt(alpha_t[time_steps]).view(-1, 1, 1, 1)  # (B, 1, 1, 1)\n",
        "  sqrt_one_minus_alpha = torch.sqrt(1 - alpha_t[time_steps]).view(-1, 1, 1, 1)\n",
        "\n",
        "  return (sqrt_alpha*images + sqrt_one_minus_alpha*noise), noise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bd-b5OFq4uci"
      },
      "source": [
        "#Time Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mn6teJ324w7C"
      },
      "outputs": [],
      "source": [
        "def get_time_embedding(time_steps, temb_dim):\n",
        "    \"\"\"\n",
        "    Convert time steps tensor into an embedding using the\n",
        "    sinusoidal time embedding formula\n",
        "    :param time_steps: 1D tensor of length batch size\n",
        "    :param temb_dim: Dimension of the embedding\n",
        "    :return: BxD embedding representation of B time steps\n",
        "    \"\"\"\n",
        "    assert temb_dim % 2 == 0, \"time embedding dimension must be divisible by 2\"\n",
        "\n",
        "    factor = 10000 ** ((torch.arange(start=0, end=temb_dim // 2, dtype=torch.float32, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")) / (temb_dim // 2))\n",
        "    )\n",
        "\n",
        "    # pos / factor\n",
        "    # timesteps B -> B, 1 -> B, temb_dim\n",
        "    t_emb = time_steps[:, None].repeat(1, temb_dim // 2) / factor\n",
        "    t_emb = torch.cat([torch.sin(t_emb), torch.cos(t_emb)], dim=-1)\n",
        "    return t_emb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT2hOuY034i6"
      },
      "source": [
        "#UNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uq8-3EBg36i-"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, emb_dim):\n",
        "    super().__init__()\n",
        "\n",
        "    self.conv1 = nn.Sequential(nn.GroupNorm(8, in_channels),\n",
        "                                 nn.SiLU(),\n",
        "                                 nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1))\n",
        "\n",
        "    self.t_emb_layer = nn.Sequential(nn.SiLU(),\n",
        "                                     nn.Linear(emb_dim, out_channels))#check this linear\n",
        "    self.word_emb_layer = nn.Sequential(nn.SiLU(),\n",
        "                                        nn.Linear(emb_dim, out_channels))\n",
        "\n",
        "    self.conv2 = nn.Sequential(nn.GroupNorm(8, out_channels),\n",
        "                                 nn.SiLU(),\n",
        "                                 nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1))\n",
        "    #for adding residual\n",
        "    self.conv_res = nn.Sequential(nn.GroupNorm(8, in_channels),\n",
        "                                    nn.SiLU(),\n",
        "                                    nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1))\n",
        "\n",
        "  def forward(self,x,t_emb, word_emb):\n",
        "    output = self.conv1(x)\n",
        "    output = output + self.t_emb_layer(t_emb).unsqueeze(-1).unsqueeze(-1) + self.word_emb_layer(word_emb).unsqueeze(-1).unsqueeze(-1) #just check embedding and this line\n",
        "    output = self.conv2(output)\n",
        "    output = output + self.conv_res(x)\n",
        "    return output\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "  def __init__(self, channels):\n",
        "    super().__init__()\n",
        "    self.norm = nn.GroupNorm(8, channels)\n",
        "    self.attention = nn.MultiheadAttention(channels, 4, batch_first=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size, channels, h, w = x.shape\n",
        "    in_attn = x.reshape(batch_size, channels, h * w)\n",
        "    in_attn = self.norm(in_attn)\n",
        "    in_attn = in_attn.transpose(1, 2)\n",
        "    out_attn, _ = self.attention(in_attn, in_attn, in_attn)\n",
        "    out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
        "    output = x + out_attn #add residual\n",
        "    return output\n",
        "\n",
        "class DownBlock(nn.Module):\n",
        "  #consists of resnetblock attention block and downsample block\n",
        "  #Return a feature map with halve the height and width and out_channels channels\n",
        "\n",
        "  def __init__(self, in_channels,out_channels, t_emb_dim):\n",
        "    super().__init__()\n",
        "    self.resBlock = ResidualBlock(in_channels, out_channels, t_emb_dim)\n",
        "    self.Attn = AttentionBlock(out_channels)\n",
        "    self.downsample = nn.Conv2d(out_channels, out_channels, kernel_size=4, stride=2,padding=1)\n",
        "\n",
        "  def forward(self,x,t_emb, word_emb):\n",
        "    output = self.resBlock(x, t_emb, word_emb)\n",
        "    output = self.Attn(output)\n",
        "    downsampled_output = self.downsample(output)\n",
        "    return output, downsampled_output  #2 resuts one used for skip connection other for assing to next layer\n",
        "\n",
        "class BottleNeckBlock(nn.Module):\n",
        "  #consists of resnetblock attention block and another resnet block\n",
        "  #Return a feature map with the same shape\n",
        "\n",
        "  def __init__(self, channels, t_emb_dim):\n",
        "    super().__init__()\n",
        "    self.resBlock = ResidualBlock(channels, channels, t_emb_dim)\n",
        "    self.Attn = AttentionBlock(channels)\n",
        "\n",
        "  def forward(self,x,t_emb, word_emb):\n",
        "    output = self.resBlock(x, t_emb, word_emb)\n",
        "    output = self.Attn(output)\n",
        "    output = self.resBlock(output, t_emb, word_emb)\n",
        "    return output\n",
        "\n",
        "class UpBlock(nn.Module):\n",
        "  #consists of resnetblock attention block and upample block\n",
        "  #Return a feature map with twice the height and width and out_channels channels\n",
        "\n",
        "  def __init__(self, in_channels,out_channels, emb_dim):\n",
        "    super().__init__()\n",
        "    self.resBlock = ResidualBlock(2*in_channels, out_channels, emb_dim) #2* for skip connection\n",
        "    self.Attn = AttentionBlock(out_channels)\n",
        "    self.upsample = nn.ConvTranspose2d(in_channels,in_channels,kernel_size=4,stride=2,padding=1)\n",
        "\n",
        "  def forward(self,x, skip, t_emb, word_emb):\n",
        "    output = self.upsample(x)\n",
        "    assert output.shape[1] == skip.shape[1], \"Skip and upsampled channels must match\"\n",
        "    output = torch.cat([output, skip], dim=1)\n",
        "    output = self.resBlock(output, t_emb, word_emb)\n",
        "    output = self.Attn(output)\n",
        "    return output\n",
        "\n",
        "class UNet(nn.Module):\n",
        "  def __init__(self, emb_dim, in_channels=3, out_channels=3):\n",
        "    super().__init__()\n",
        "    channels = [64,128,256]\n",
        "    self.emb_dim = emb_dim\n",
        "\n",
        "    self.first_layer = nn.Conv2d(in_channels, 32, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    self.down1 = DownBlock(32,channels[0], emb_dim)\n",
        "\n",
        "    self.down2 = DownBlock(channels[0],channels[1], emb_dim)\n",
        "    self.down3 = DownBlock(channels[1],channels[2], emb_dim)\n",
        "\n",
        "    self.bottleneck = BottleNeckBlock(channels[2], emb_dim)\n",
        "\n",
        "    self.up1 = UpBlock(channels[2],channels[1], emb_dim)\n",
        "    self.up2 = UpBlock(channels[1],channels[0], emb_dim)\n",
        "    self.up3 = UpBlock(channels[0],channels[0]//2, emb_dim)\n",
        "\n",
        "    self.final_conv = nn.Sequential(nn.GroupNorm(8, channels[0]//2),\n",
        "                                    nn.SiLU(),\n",
        "                                    nn.Conv2d(channels[0]//2, out_channels, kernel_size=3, stride=1, padding=1))\n",
        "\n",
        "    self.word_emb = nn.Embedding(11,self.emb_dim)\n",
        "\n",
        "\n",
        "  def forward(self, x, t, label):\n",
        "    #emb\n",
        "    t_emb = get_time_embedding(t, self.emb_dim)\n",
        "    word_emb = self.word_emb(label)\n",
        "\n",
        "    x= self.first_layer(x)\n",
        "\n",
        "    skip1, x1 = self.down1(x, t_emb, word_emb)\n",
        "    skip2, x2 = self.down2(x1,t_emb, word_emb)\n",
        "    skip3, x3 = self.down3(x2, t_emb, word_emb)\n",
        "\n",
        "    x4 = self.bottleneck(x3, t_emb, word_emb)\n",
        "\n",
        "    x5 = self.up1(x4, skip3, t_emb, word_emb)\n",
        "    x6 = self.up2(x5, skip2, t_emb, word_emb)\n",
        "    x7 = self.up3(x6, skip1, t_emb, word_emb)\n",
        "\n",
        "    out = self.final_conv(x7)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#this code loads a save statedict to reiniialise a trained model\n",
        "from pathlib import Path\n",
        "import os\n",
        "MODEL_PATH = Path(\"models\")\n",
        "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "MODEL_NAME = \"Unet.pt\"\n",
        "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME"
      ],
      "metadata": {
        "id": "m46WQQMxveyC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4feg0x9n-Gcs",
        "outputId": "b332cb9d-756a-4d9a-88bb-243f47533191"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "UNet                                     --                        --\n",
              "├─Embedding: 1-1                         [1, 24]                   264\n",
              "├─Conv2d: 1-2                            [1, 32, 32, 32]           896\n",
              "├─DownBlock: 1-3                         [1, 64, 32, 32]           --\n",
              "│    └─ResidualBlock: 2-1                [1, 64, 32, 32]           --\n",
              "│    │    └─Sequential: 3-1              [1, 64, 32, 32]           18,560\n",
              "│    │    └─Sequential: 3-2              [1, 64]                   1,600\n",
              "│    │    └─Sequential: 3-3              [1, 64]                   1,600\n",
              "│    │    └─Sequential: 3-4              [1, 64, 32, 32]           37,056\n",
              "│    │    └─Sequential: 3-5              [1, 64, 32, 32]           2,176\n",
              "│    └─AttentionBlock: 2-2               [1, 64, 32, 32]           --\n",
              "│    │    └─GroupNorm: 3-6               [1, 64, 1024]             128\n",
              "│    │    └─MultiheadAttention: 3-7      [1, 1024, 64]             16,640\n",
              "│    └─Conv2d: 2-3                       [1, 64, 16, 16]           65,600\n",
              "├─DownBlock: 1-4                         [1, 128, 16, 16]          --\n",
              "│    └─ResidualBlock: 2-4                [1, 128, 16, 16]          --\n",
              "│    │    └─Sequential: 3-8              [1, 128, 16, 16]          73,984\n",
              "│    │    └─Sequential: 3-9              [1, 128]                  3,200\n",
              "│    │    └─Sequential: 3-10             [1, 128]                  3,200\n",
              "│    │    └─Sequential: 3-11             [1, 128, 16, 16]          147,840\n",
              "│    │    └─Sequential: 3-12             [1, 128, 16, 16]          8,448\n",
              "│    └─AttentionBlock: 2-5               [1, 128, 16, 16]          --\n",
              "│    │    └─GroupNorm: 3-13              [1, 128, 256]             256\n",
              "│    │    └─MultiheadAttention: 3-14     [1, 256, 128]             66,048\n",
              "│    └─Conv2d: 2-6                       [1, 128, 8, 8]            262,272\n",
              "├─DownBlock: 1-5                         [1, 256, 8, 8]            --\n",
              "│    └─ResidualBlock: 2-7                [1, 256, 8, 8]            --\n",
              "│    │    └─Sequential: 3-15             [1, 256, 8, 8]            295,424\n",
              "│    │    └─Sequential: 3-16             [1, 256]                  6,400\n",
              "│    │    └─Sequential: 3-17             [1, 256]                  6,400\n",
              "│    │    └─Sequential: 3-18             [1, 256, 8, 8]            590,592\n",
              "│    │    └─Sequential: 3-19             [1, 256, 8, 8]            33,280\n",
              "│    └─AttentionBlock: 2-8               [1, 256, 8, 8]            --\n",
              "│    │    └─GroupNorm: 3-20              [1, 256, 64]              512\n",
              "│    │    └─MultiheadAttention: 3-21     [1, 64, 256]              263,168\n",
              "│    └─Conv2d: 2-9                       [1, 256, 4, 4]            1,048,832\n",
              "├─BottleNeckBlock: 1-6                   [1, 256, 4, 4]            --\n",
              "│    └─ResidualBlock: 2-10               [1, 256, 4, 4]            --\n",
              "│    │    └─Sequential: 3-22             [1, 256, 4, 4]            590,592\n",
              "│    │    └─Sequential: 3-23             [1, 256]                  6,400\n",
              "│    │    └─Sequential: 3-24             [1, 256]                  6,400\n",
              "│    │    └─Sequential: 3-25             [1, 256, 4, 4]            590,592\n",
              "│    │    └─Sequential: 3-26             [1, 256, 4, 4]            66,304\n",
              "│    └─AttentionBlock: 2-11              [1, 256, 4, 4]            --\n",
              "│    │    └─GroupNorm: 3-27              [1, 256, 16]              512\n",
              "│    │    └─MultiheadAttention: 3-28     [1, 16, 256]              263,168\n",
              "│    └─ResidualBlock: 2-12               [1, 256, 4, 4]            (recursive)\n",
              "│    │    └─Sequential: 3-29             [1, 256, 4, 4]            (recursive)\n",
              "│    │    └─Sequential: 3-30             [1, 256]                  (recursive)\n",
              "│    │    └─Sequential: 3-31             [1, 256]                  (recursive)\n",
              "│    │    └─Sequential: 3-32             [1, 256, 4, 4]            (recursive)\n",
              "│    │    └─Sequential: 3-33             [1, 256, 4, 4]            (recursive)\n",
              "├─UpBlock: 1-7                           [1, 128, 8, 8]            --\n",
              "│    └─ConvTranspose2d: 2-13             [1, 256, 8, 8]            1,048,832\n",
              "│    └─ResidualBlock: 2-14               [1, 128, 8, 8]            --\n",
              "│    │    └─Sequential: 3-34             [1, 128, 8, 8]            590,976\n",
              "│    │    └─Sequential: 3-35             [1, 128]                  3,200\n",
              "│    │    └─Sequential: 3-36             [1, 128]                  3,200\n",
              "│    │    └─Sequential: 3-37             [1, 128, 8, 8]            147,840\n",
              "│    │    └─Sequential: 3-38             [1, 128, 8, 8]            66,688\n",
              "│    └─AttentionBlock: 2-15              [1, 128, 8, 8]            --\n",
              "│    │    └─GroupNorm: 3-39              [1, 128, 64]              256\n",
              "│    │    └─MultiheadAttention: 3-40     [1, 64, 128]              66,048\n",
              "├─UpBlock: 1-8                           [1, 64, 16, 16]           --\n",
              "│    └─ConvTranspose2d: 2-16             [1, 128, 16, 16]          262,272\n",
              "│    └─ResidualBlock: 2-17               [1, 64, 16, 16]           --\n",
              "│    │    └─Sequential: 3-41             [1, 64, 16, 16]           148,032\n",
              "│    │    └─Sequential: 3-42             [1, 64]                   1,600\n",
              "│    │    └─Sequential: 3-43             [1, 64]                   1,600\n",
              "│    │    └─Sequential: 3-44             [1, 64, 16, 16]           37,056\n",
              "│    │    └─Sequential: 3-45             [1, 64, 16, 16]           16,960\n",
              "│    └─AttentionBlock: 2-18              [1, 64, 16, 16]           --\n",
              "│    │    └─GroupNorm: 3-46              [1, 64, 256]              128\n",
              "│    │    └─MultiheadAttention: 3-47     [1, 256, 64]              16,640\n",
              "├─UpBlock: 1-9                           [1, 32, 32, 32]           --\n",
              "│    └─ConvTranspose2d: 2-19             [1, 64, 32, 32]           65,600\n",
              "│    └─ResidualBlock: 2-20               [1, 32, 32, 32]           --\n",
              "│    │    └─Sequential: 3-48             [1, 32, 32, 32]           37,152\n",
              "│    │    └─Sequential: 3-49             [1, 32]                   800\n",
              "│    │    └─Sequential: 3-50             [1, 32]                   800\n",
              "│    │    └─Sequential: 3-51             [1, 32, 32, 32]           9,312\n",
              "│    │    └─Sequential: 3-52             [1, 32, 32, 32]           4,384\n",
              "│    └─AttentionBlock: 2-21              [1, 32, 32, 32]           --\n",
              "│    │    └─GroupNorm: 3-53              [1, 32, 1024]             64\n",
              "│    │    └─MultiheadAttention: 3-54     [1, 1024, 32]             4,224\n",
              "├─Sequential: 1-10                       [1, 3, 32, 32]            --\n",
              "│    └─GroupNorm: 2-22                   [1, 32, 32, 32]           64\n",
              "│    └─SiLU: 2-23                        [1, 32, 32, 32]           --\n",
              "│    └─Conv2d: 2-24                      [1, 3, 32, 32]            867\n",
              "==========================================================================================\n",
              "Total params: 7,012,939\n",
              "Trainable params: 7,012,939\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 624.37\n",
              "==========================================================================================\n",
              "Input size (MB): 0.01\n",
              "Forward/backward pass size (MB): 13.61\n",
              "Params size (MB): 25.27\n",
              "Estimated Total Size (MB): 38.89\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#gives a top down give of model architecture using torchinfo\n",
        "model=UNet(24)\n",
        "try:\n",
        "  import torchinfo\n",
        "except:\n",
        "  !pip install torchinfo\n",
        "  import torchinfo\n",
        "\n",
        "B = 1\n",
        "input_image = (B, 3, 32, 32)\n",
        "time_tensor = torch.tensor([10.0] * B)  # Example time\n",
        "label_tensor = torch.tensor([1] * B)    # Example label\n",
        "\n",
        "# You must pass a tuple of example inputs\n",
        "torchinfo.summary(model, input_data=(torch.randn(input_image), time_tensor, label_tensor))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sldGyFWt5aBP"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "C-XpSeauHXum",
        "outputId": "24a293be-9aed-43be-dd8c-106ae1beee3e"
      },
      "outputs": [
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 1.50 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.22 GiB is free. Process 2493 has 13.52 GiB memory in use. Of the allocated memory 11.35 GiB is allocated by PyTorch, and 2.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-2676860510>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mepoch_loss\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.50 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.22 GiB is free. Process 2493 has 13.52 GiB memory in use. Of the allocated memory 11.35 GiB is allocated by PyTorch, and 2.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "batch_size=96\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    combined_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,\n",
        "    drop_last=True)\n",
        "\n",
        "epochs = 30\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = UNet(24).to(DEVICE)\n",
        "if os.path.exists(MODEL_SAVE_PATH):\n",
        "  model.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "for i in range(epochs):\n",
        "  model.train()\n",
        "  epoch_loss = 0.0\n",
        "  count=0\n",
        "\n",
        "  for (images, labels) in train_loader:\n",
        "    images = images.to(DEVICE, non_blocking=True)\n",
        "    labels = labels.to(DEVICE, non_blocking=True)\n",
        "\n",
        "        # Prepare batch with CFG\n",
        "    timesteps = torch.randint(0, 400, (images.size(0),), device=DEVICE)\n",
        "    labels[torch.rand(images.size(0), device=DEVICE) < 0.2] = 10\n",
        "        # Forward pass\n",
        "    model_input, target_noise = forward_diff(images, timesteps)\n",
        "\n",
        "    pred_noise = model(model_input, timesteps, labels)\n",
        "\n",
        "        # Loss and backward\n",
        "    loss = criterion(pred_noise, target_noise)\n",
        "    epoch_loss+=loss\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  print(f\"epoch: {i}\")\n",
        "  print(f\"epoch loss {epoch_loss/len(train_loader)}\")\n",
        "  if i%2==0:\n",
        "    torch.save(obj=model.state_dict(), f=MODEL_SAVE_PATH)\n",
        "    print(\"saved\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MP5bcM4SNhr"
      },
      "source": [
        "#Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dwdOV8FSLMH"
      },
      "outputs": [],
      "source": [
        "def generate(label, cfg_w=4.5):\n",
        "  DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  label = torch.tensor([label],device=DEVICE)\n",
        "\n",
        "  model = UNet(24).to(DEVICE)\n",
        "  if os.path.exists(MODEL_SAVE_PATH):\n",
        "    model.load_state_dict(torch.load(f=MODEL_SAVE_PATH, map_location=torch.device('cpu')))\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  #make random noise image\n",
        "  T = 400  # Total timesteps\n",
        "  noisy_image = torch.randn(1, 3, 32, 32, device=DEVICE)  # Start from pure noise\n",
        "\n",
        "  s = 0.08\n",
        "  t_vals = torch.linspace(0, T, T + 1, device=DEVICE)\n",
        "  alpha_t = torch.cos((t_vals / T + s) / (1 + s) * torch.pi / 2) ** 2\n",
        "  alpha_t = alpha_t / alpha_t[0]\n",
        "  beta_t = 1 - (alpha_t[1:] / alpha_t[:-1])\n",
        "  beta_t = torch.clip(beta_t, 1e-4, 0.02)\n",
        "  alpha_t = 1 - beta_t\n",
        "  alpha_cumprod = torch.cumprod(alpha_t, dim=0)\n",
        "\n",
        "  with torch.inference_mode():\n",
        "    for t in reversed(range(T)):\n",
        "      t_tensor = torch.tensor([t], device=DEVICE)\n",
        "\n",
        "      pred_noise_cond = model(noisy_image, t_tensor, label)\n",
        "      pred_noise_uncond = model(noisy_image, t_tensor, torch.tensor([10], device=DEVICE))\n",
        "\n",
        "      # Classifier-free guidance\n",
        "      pred_noise = (pred_noise_uncond + cfg_w * (pred_noise_cond - pred_noise_uncond))\n",
        "\n",
        "      # DDPM reverse update\n",
        "      sqrt_recip_alpha_t = 1.0 / torch.sqrt(alpha_t[t])\n",
        "      sqrt_one_minus_alpha_cumprod = torch.sqrt(1 - alpha_cumprod[t])\n",
        "      coeff = (1 - alpha_t[t]) / sqrt_one_minus_alpha_cumprod\n",
        "\n",
        "      noisy_image = sqrt_recip_alpha_t * (noisy_image - coeff * pred_noise)\n",
        "\n",
        "      if t > 0:\n",
        "        noise = torch.randn_like(noisy_image)\n",
        "        noisy_image += torch.sqrt(beta_t[t]) * noise\n",
        "\n",
        "      #print(noisy_image[0, 0, :, 0])  # optional debug\n",
        "\n",
        "    # Clamp to valid range\n",
        "    return torch.clamp(noisy_image, -1, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YW27VroZY4i_",
        "outputId": "0f01de41-091d-4828-c6e3-42d161ded652"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/Unet1 (4) (8).pt\n"
          ]
        }
      ],
      "source": [
        "  #this function generates an image and converts it to matplotlib readable format before displaying\n",
        "def show(lable,y):\n",
        "  x = generate(lable, y)\n",
        "  image = x.detach().cpu()\n",
        "  image = image.squeeze(0)\n",
        "  image = (image + 1) / 2\n",
        "  image = image.permute(1, 2, 0).numpy()\n",
        "  plt.figure(figsize=(8, 8))\n",
        "  plt.imshow(image)\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688
        },
        "id": "4l2oe_tFsctq",
        "outputId": "8a59ad2d-7165-42ab-a8d7-9b2a84f62848"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading\n",
            "Finished loading\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "#example\n",
        "show(7,4.5)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "cYdzxvNXpNT4"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}