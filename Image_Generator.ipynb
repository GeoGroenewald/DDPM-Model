{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZL2qgW2BLXcY"
      },
      "source": [
        "# Data loading and diffusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "olk9Xz42LSur"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import ConcatDataset, DataLoader\n",
        "import torch\n",
        "import torchvision.utils as vutils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbivIoqaVUU9"
      },
      "source": [
        "##Load labelled data from STL10 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "togXHo4GwWk5"
      },
      "outputs": [],
      "source": [
        "\n",
        "transform = transforms.Compose([\n",
        "transforms.Resize((32, 32)),             # Resize any image to 96x96\n",
        "transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='.', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.CIFAR10(root='.', train=False, transform=transform, download=True)\n",
        "combined_dataset = ConcatDataset([train_dataset, test_dataset])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBKA_2xE2pfA"
      },
      "outputs": [],
      "source": [
        "x, __ = combined_dataset[0]\n",
        "testing = torch.unsqueeze(x,0)\n",
        "testing.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BP_3xBIVc-c"
      },
      "source": [
        "## Define all values and functions needed to diffuse a image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SpjNRreZLWBE"
      },
      "outputs": [],
      "source": [
        "#Define some diffusion schedule values that will be used in the diffusion forward pass\n",
        "#These values are computed uaing the cosine schedule\n",
        "T = 400\n",
        "s = 0.08\n",
        "t = torch.linspace(0,T,T+1, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
        "alpha_t = torch.cos((t/T + s)/(1 + s) * (torch.pi / 2)) ** 2\n",
        "alpha_t = alpha_t / alpha_t[0]\n",
        "beta_t = 1 - (alpha_t[1:] / alpha_t[:-1])  # β[1], β[2], ..., β[T]\n",
        "beta_t = torch.clip(beta_t, 1e-4, 0.999)\n",
        "beta_t = beta_t.type(torch.float32)\n",
        "alpha_cumprod = torch.cumprod(1-beta_t,dim=0)\n",
        "\n",
        "def forward_diff(images, time_steps):\n",
        "  #This function performs the diffusion of a normal recognizable batch of images and returns the diffused image as well as the noise for training\n",
        "  #this model uses the cosine schedule\n",
        "  #time step is used to define the diffusion at certain time steps\n",
        "\n",
        "  noise = torch.randn_like(images, device=images.device)\n",
        "\n",
        "  sqrt_alpha = torch.sqrt(alpha_t[time_steps]).view(-1, 1, 1, 1)  # (B, 1, 1, 1)\n",
        "  sqrt_one_minus_alpha = torch.sqrt(1 - alpha_t[time_steps]).view(-1, 1, 1, 1)\n",
        "\n",
        "  return (sqrt_alpha*images + sqrt_one_minus_alpha*noise), noise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bd-b5OFq4uci"
      },
      "source": [
        "#Time Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mn6teJ324w7C"
      },
      "outputs": [],
      "source": [
        "def get_time_embedding(time_steps, temb_dim):\n",
        "    \"\"\"\n",
        "    Convert time steps tensor into an embedding using the\n",
        "    sinusoidal time embedding formula\n",
        "    :param time_steps: 1D tensor of length batch size\n",
        "    :param temb_dim: Dimension of the embedding\n",
        "    :return: BxD embedding representation of B time steps\n",
        "    \"\"\"\n",
        "    assert temb_dim % 2 == 0, \"time embedding dimension must be divisible by 2\"\n",
        "\n",
        "    factor = 10000 ** ((torch.arange(start=0, end=temb_dim // 2, dtype=torch.float32, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")) / (temb_dim // 2))\n",
        "    )\n",
        "\n",
        "    # pos / factor\n",
        "    # timesteps B -> B, 1 -> B, temb_dim\n",
        "    t_emb = time_steps[:, None].repeat(1, temb_dim // 2) / factor\n",
        "    t_emb = torch.cat([torch.sin(t_emb), torch.cos(t_emb)], dim=-1)\n",
        "    return t_emb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT2hOuY034i6"
      },
      "source": [
        "#UNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uq8-3EBg36i-"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, emb_dim):\n",
        "    super().__init__()\n",
        "\n",
        "    self.conv1 = nn.Sequential(nn.GroupNorm(8, in_channels),\n",
        "                                 nn.SiLU(),\n",
        "                                 nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1))\n",
        "\n",
        "    self.t_emb_layer = nn.Sequential(nn.SiLU(),\n",
        "                                     nn.Linear(emb_dim, out_channels))#check this linear\n",
        "    self.word_emb_layer = nn.Sequential(nn.SiLU(),\n",
        "                                        nn.Linear(emb_dim, out_channels))\n",
        "\n",
        "    self.conv2 = nn.Sequential(nn.GroupNorm(8, out_channels),\n",
        "                                 nn.SiLU(),\n",
        "                                 nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1))\n",
        "    #for adding residual\n",
        "    self.conv_res = nn.Sequential(nn.GroupNorm(8, in_channels),\n",
        "                                    nn.SiLU(),\n",
        "                                    nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1))\n",
        "\n",
        "  def forward(self,x,t_emb, word_emb):\n",
        "    output = self.conv1(x)\n",
        "    output = output + self.t_emb_layer(t_emb).unsqueeze(-1).unsqueeze(-1) + self.word_emb_layer(word_emb).unsqueeze(-1).unsqueeze(-1) #just check embedding and this line\n",
        "    output = self.conv2(output)\n",
        "    output = output + self.conv_res(x)\n",
        "    return output\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "  def __init__(self, channels):\n",
        "    super().__init__()\n",
        "    self.norm = nn.GroupNorm(8, channels)\n",
        "    self.attention = nn.MultiheadAttention(channels, 4, batch_first=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size, channels, h, w = x.shape\n",
        "    in_attn = x.reshape(batch_size, channels, h * w)\n",
        "    in_attn = self.norm(in_attn)\n",
        "    in_attn = in_attn.transpose(1, 2)\n",
        "    out_attn, _ = self.attention(in_attn, in_attn, in_attn)\n",
        "    out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
        "    output = x + out_attn #add residual\n",
        "    return output\n",
        "\n",
        "class DownBlock(nn.Module):\n",
        "  #consists of resnetblock attention block and downsample block\n",
        "  #Return a feature map with halve the height and width and out_channels channels\n",
        "\n",
        "  def __init__(self, in_channels,out_channels, t_emb_dim):\n",
        "    super().__init__()\n",
        "    self.resBlock = ResidualBlock(in_channels, out_channels, t_emb_dim)\n",
        "    self.Attn = AttentionBlock(out_channels)\n",
        "    self.downsample = nn.Conv2d(out_channels, out_channels, kernel_size=4, stride=2,padding=1)\n",
        "\n",
        "  def forward(self,x,t_emb, word_emb):\n",
        "    output = self.resBlock(x, t_emb, word_emb)\n",
        "    output = self.Attn(output)\n",
        "    downsampled_output = self.downsample(output)\n",
        "    return output, downsampled_output  #2 resuts one used for skip connection other for assing to next layer\n",
        "\n",
        "class BottleNeckBlock(nn.Module):\n",
        "  #consists of resnetblock attention block and another resnet block\n",
        "  #Return a feature map with the same shape\n",
        "\n",
        "  def __init__(self, channels, t_emb_dim):\n",
        "    super().__init__()\n",
        "    self.resBlock = ResidualBlock(channels, channels, t_emb_dim)\n",
        "    self.Attn = AttentionBlock(channels)\n",
        "\n",
        "  def forward(self,x,t_emb, word_emb):\n",
        "    output = self.resBlock(x, t_emb, word_emb)\n",
        "    output = self.Attn(output)\n",
        "    output = self.resBlock(output, t_emb, word_emb)\n",
        "    return output\n",
        "\n",
        "class UpBlock(nn.Module):\n",
        "  #consists of resnetblock attention block and upample block\n",
        "  #Return a feature map with twice the height and width and out_channels channels\n",
        "\n",
        "  def __init__(self, in_channels,out_channels, emb_dim):\n",
        "    super().__init__()\n",
        "    self.resBlock = ResidualBlock(2*in_channels, out_channels, emb_dim) #2* for skip connection\n",
        "    self.Attn = AttentionBlock(out_channels)\n",
        "    self.upsample = nn.ConvTranspose2d(in_channels,in_channels,kernel_size=4,stride=2,padding=1)\n",
        "\n",
        "  def forward(self,x, skip, t_emb, word_emb):\n",
        "    output = self.upsample(x)\n",
        "    assert output.shape[1] == skip.shape[1], \"Skip and upsampled channels must match\"\n",
        "    output = torch.cat([output, skip], dim=1)\n",
        "    output = self.resBlock(output, t_emb, word_emb)\n",
        "    output = self.Attn(output)\n",
        "    return output\n",
        "\n",
        "class UNet(nn.Module):\n",
        "  def __init__(self, emb_dim, in_channels=3, out_channels=3):\n",
        "    super().__init__()\n",
        "    channels = [64,128,256]\n",
        "    self.emb_dim = emb_dim\n",
        "\n",
        "    self.first_layer = nn.Conv2d(in_channels, 32, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    self.down1 = DownBlock(32,channels[0], emb_dim)\n",
        "\n",
        "    self.down2 = DownBlock(channels[0],channels[1], emb_dim)\n",
        "    self.down3 = DownBlock(channels[1],channels[2], emb_dim)\n",
        "\n",
        "    self.bottleneck = BottleNeckBlock(channels[2], emb_dim)\n",
        "\n",
        "    self.up1 = UpBlock(channels[2],channels[1], emb_dim)\n",
        "    self.up2 = UpBlock(channels[1],channels[0], emb_dim)\n",
        "    self.up3 = UpBlock(channels[0],channels[0]//2, emb_dim)\n",
        "\n",
        "    self.final_conv = nn.Sequential(nn.GroupNorm(8, channels[0]//2),\n",
        "                                    nn.SiLU(),\n",
        "                                    nn.Conv2d(channels[0]//2, out_channels, kernel_size=3, stride=1, padding=1))\n",
        "\n",
        "    self.word_emb = nn.Embedding(11,self.emb_dim)\n",
        "\n",
        "\n",
        "  def forward(self, x, t, label):\n",
        "    #emb\n",
        "    t_emb = get_time_embedding(t, self.emb_dim)\n",
        "    word_emb = self.word_emb(label)\n",
        "\n",
        "    x= self.first_layer(x)\n",
        "\n",
        "    skip1, x1 = self.down1(x, t_emb, word_emb)\n",
        "    skip2, x2 = self.down2(x1,t_emb, word_emb)\n",
        "    skip3, x3 = self.down3(x2, t_emb, word_emb)\n",
        "\n",
        "    x4 = self.bottleneck(x3, t_emb, word_emb)\n",
        "\n",
        "    x5 = self.up1(x4, skip3, t_emb, word_emb)\n",
        "    x6 = self.up2(x5, skip2, t_emb, word_emb)\n",
        "    x7 = self.up3(x6, skip1, t_emb, word_emb)\n",
        "\n",
        "    out = self.final_conv(x7)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#this code loads a save statedict to reiniialise a trained model\n",
        "from pathlib import Path\n",
        "import os\n",
        "MODEL_PATH = Path(\"models\")\n",
        "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "MODEL_NAME = \"Unet.pt\"\n",
        "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME"
      ],
      "metadata": {
        "id": "m46WQQMxveyC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4feg0x9n-Gcs"
      },
      "outputs": [],
      "source": [
        "#gives a top down give of model architecture using torchinfo\n",
        "model=UNet(24)\n",
        "try:\n",
        "  import torchinfo\n",
        "except:\n",
        "  !pip install torchinfo\n",
        "  import torchinfo\n",
        "\n",
        "B = 1\n",
        "input_image = (B, 3, 32, 32)\n",
        "time_tensor = torch.tensor([10.0] * B)  # Example time\n",
        "label_tensor = torch.tensor([1] * B)    # Example label\n",
        "\n",
        "# You must pass a tuple of example inputs\n",
        "torchinfo.summary(model, input_data=(torch.randn(input_image), time_tensor, label_tensor))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sldGyFWt5aBP"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-XpSeauHXum"
      },
      "outputs": [],
      "source": [
        "batch_size=96\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    combined_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,\n",
        "    drop_last=True)\n",
        "\n",
        "epochs = 30\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = UNet(24).to(DEVICE)\n",
        "if os.path.exists(MODEL_SAVE_PATH):\n",
        "  model.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "for i in range(epochs):\n",
        "  model.train()\n",
        "  epoch_loss = 0.0\n",
        "  count=0\n",
        "\n",
        "  for (images, labels) in train_loader:\n",
        "    images = images.to(DEVICE, non_blocking=True)\n",
        "    labels = labels.to(DEVICE, non_blocking=True)\n",
        "\n",
        "        # Prepare batch with CFG\n",
        "    timesteps = torch.randint(0, 400, (images.size(0),), device=DEVICE)\n",
        "    labels[torch.rand(images.size(0), device=DEVICE) < 0.2] = 10\n",
        "        # Forward pass\n",
        "    model_input, target_noise = forward_diff(images, timesteps)\n",
        "\n",
        "    pred_noise = model(model_input, timesteps, labels)\n",
        "\n",
        "        # Loss and backward\n",
        "    loss = criterion(pred_noise, target_noise)\n",
        "    epoch_loss+=loss\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  print(f\"epoch: {i}\")\n",
        "  print(f\"epoch loss {epoch_loss/len(train_loader)}\")\n",
        "  if i%2==0:\n",
        "    torch.save(obj=model.state_dict(), f=MODEL_SAVE_PATH)\n",
        "    print(\"saved\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MP5bcM4SNhr"
      },
      "source": [
        "#Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dwdOV8FSLMH"
      },
      "outputs": [],
      "source": [
        "def generate(label, cfg_w=4.5):\n",
        "  DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  label = torch.tensor([label],device=DEVICE)\n",
        "\n",
        "  model = UNet(24).to(DEVICE)\n",
        "  if os.path.exists(MODEL_SAVE_PATH):\n",
        "    model.load_state_dict(torch.load(f=MODEL_SAVE_PATH, map_location=torch.device('cpu')))\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  #make random noise image\n",
        "  T = 400  # Total timesteps\n",
        "  noisy_image = torch.randn(1, 3, 32, 32, device=DEVICE)  # Start from pure noise\n",
        "\n",
        "  s = 0.08\n",
        "  t_vals = torch.linspace(0, T, T + 1, device=DEVICE)\n",
        "  alpha_t = torch.cos((t_vals / T + s) / (1 + s) * torch.pi / 2) ** 2\n",
        "  alpha_t = alpha_t / alpha_t[0]\n",
        "  beta_t = 1 - (alpha_t[1:] / alpha_t[:-1])\n",
        "  beta_t = torch.clip(beta_t, 1e-4, 0.02)\n",
        "  alpha_t = 1 - beta_t\n",
        "  alpha_cumprod = torch.cumprod(alpha_t, dim=0)\n",
        "\n",
        "  with torch.inference_mode():\n",
        "    for t in reversed(range(T)):\n",
        "      t_tensor = torch.tensor([t], device=DEVICE)\n",
        "\n",
        "      pred_noise_cond = model(noisy_image, t_tensor, label)\n",
        "      pred_noise_uncond = model(noisy_image, t_tensor, torch.tensor([10], device=DEVICE))\n",
        "\n",
        "      # Classifier-free guidance\n",
        "      pred_noise = (pred_noise_uncond + cfg_w * (pred_noise_cond - pred_noise_uncond))\n",
        "\n",
        "      # DDPM reverse update\n",
        "      sqrt_recip_alpha_t = 1.0 / torch.sqrt(alpha_t[t])\n",
        "      sqrt_one_minus_alpha_cumprod = torch.sqrt(1 - alpha_cumprod[t])\n",
        "      coeff = (1 - alpha_t[t]) / sqrt_one_minus_alpha_cumprod\n",
        "\n",
        "      noisy_image = sqrt_recip_alpha_t * (noisy_image - coeff * pred_noise)\n",
        "\n",
        "      if t > 0:\n",
        "        noise = torch.randn_like(noisy_image)\n",
        "        noisy_image += torch.sqrt(beta_t[t]) * noise\n",
        "\n",
        "      #print(noisy_image[0, 0, :, 0])  # optional debug\n",
        "\n",
        "    # Clamp to valid range\n",
        "    return torch.clamp(noisy_image, -1, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YW27VroZY4i_"
      },
      "outputs": [],
      "source": [
        "  #this function generates an image and converts it to matplotlib readable format before displaying\n",
        "def show(lable,y):\n",
        "  x = generate(lable, y)\n",
        "  image = x.detach().cpu()\n",
        "  image = image.squeeze(0)\n",
        "  image = (image + 1) / 2\n",
        "  image = image.permute(1, 2, 0).numpy()\n",
        "  plt.figure(figsize=(8, 8))\n",
        "  plt.imshow(image)\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4l2oe_tFsctq"
      },
      "outputs": [],
      "source": [
        "#example\n",
        "show(7,4.5)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "cYdzxvNXpNT4"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}